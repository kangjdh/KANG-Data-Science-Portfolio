{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a Decision Tree Model\n",
        "\n",
        "In this notebook, we will load the Iris dataset, split it into training and test sets, and then fine-tune a Decision Tree classifier using GridSearchCV. We'll evaluate the performance of the best model using classification metrics and visualize the resulting decision tree."
      ],
      "metadata": {
        "id": "SAms_E00CSST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load and Inspect the Data**\n",
        "\n",
        "We use the Titanic dataset available from seaborn, which includes details about passengers. This dataset is widely used for classification tasks."
      ],
      "metadata": {
        "id": "lImPR-PAChUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dBYPNotuwfd",
        "outputId": "a6bddf28-8b2c-4d03-a456-7d9b3e5a4f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset in seaborn\n",
        "data = sns.load_dataset('titanic')\n",
        "\n",
        "# Inspect the dataset\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Data Preprocessing**\n",
        "\n",
        "Our next step is to prepare the data for modeling:\n",
        "\n",
        "- **Handling Missing Values:**\n",
        "   Here, the line for dropping rows with missing 'age' is commented out.\n",
        "   Depending on your needs, you might choose to drop these rows or use imputation.\n",
        "\n",
        "- **Encoding Categorical Variables:**\n",
        "   Decision tree algorithms can handle numerical inputs, so we convert\n",
        "   categorical variables (e.g., `sex`) into numeric format using one-hot encoding.\n",
        "\n",
        "*Note: We use drop_first=True to avoid the dummy variable trap.*"
      ],
      "metadata": {
        "id": "20rN0ZYCCmLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical variables\n",
        "df = pd.get_dummies(data, columns=['sex'], drop_first=True) # Use drop_first = True to avoid \"dummy trap\"\n",
        "\n",
        "# Define features and target\n",
        "features = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex_male']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Preview the cleaned dataset\n",
        "print(X.head())\n",
        "print(y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtx_7MlTvIcD",
        "outputId": "90bf3318-6fe2-488d-e134-633eed0d2bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   pclass   age  sibsp  parch     fare  sex_male\n",
            "0       3  22.0      1      0   7.2500      True\n",
            "1       1  38.0      1      0  71.2833     False\n",
            "2       3  26.0      0      0   7.9250     False\n",
            "3       1  35.0      1      0  53.1000     False\n",
            "4       3  35.0      0      0   8.0500      True\n",
            "0    0\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    0\n",
            "Name: survived, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Splitting the Data**\n",
        "\n",
        "We split the dataset into training and testing sets. The training set is used to build the decision tree model, while the testing set is used to evaluate its performance."
      ],
      "metadata": {
        "id": "3OmbakPvCzxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "yblwppiF7ya3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Fine-tuning the Decision Tree Model**\n",
        "\n",
        "We'll perform hyperparameter tuning using GridSearchCV to find the best combination of parameters for the Decision Tree classifier. The parameters we will tune include:\n",
        "\n",
        "- `criterion`: The algorithm used to optimize each split\n",
        "- `max_depth`: Maximum depth of the tree\n",
        "- `min_samples_split`: Minimum number of samples required to split an internal node\n",
        "- `min_samples_leaf`: Minimum number of samples required to be at a leaf node"
      ],
      "metadata": {
        "id": "O0CcE1FzC8bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "\n",
        "# Set up GridSearchCV\n",
        "\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "\n",
        "# Look at the best parameters and scores\n"
      ],
      "metadata": {
        "id": "n1MnU_HRu2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Evaluate the Model**\n",
        "\n",
        "Using the best estimator from the grid search, we will evaluate its performance on the test set by generating a classification report and confusion matrix."
      ],
      "metadata": {
        "id": "n0UT24sfDEpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Get the best estimator\n",
        "\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_dtree.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "lPJLnzvu8DkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Visualizing the Decision Tree**\n",
        "\n",
        "One of the advantages of decision trees is their interpretability. We can visualize the tree structure using the graphviz library.\n",
        "The visualization shows:\n",
        "- Splitting criteria at each node.\n",
        "- Feature names used for splits.\n",
        "- Class distributions within the nodes."
      ],
      "metadata": {
        "id": "rG-kkOyZDKAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import graphviz and export the decision tree to dot format for visualization\n",
        "import graphviz\n",
        "from sklearn import tree  # Ensure to import the tree module from sklearn\n",
        "\n",
        "dot_data = tree.export_graphviz(best_dtree,\n",
        "                                feature_names=X_train.columns,\n",
        "                                class_names=[\"Not_Survived\", \"Survived\"],\n",
        "                                filled=True)\n",
        "\n",
        "# Generate and display the decision tree graph\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "nXXsbI-39i_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}